{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4f6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "from urllib.parse import urljoin\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885ea852",
   "metadata": {},
   "outputs": [],
   "source": [
    " # headers\n",
    "headers = {\n",
    "    \"User-Agent\":         \n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024f1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui sâ€™assure que le crawler a le droit de parser une page\n",
    "def can_crawl(url):\n",
    "    robots_url = url + \"/robots.txt\"\n",
    "    #print(robots_url)\n",
    "    response = requests.get(robots_url)\n",
    "    if response.status_code == 200:\n",
    "        robots_txt = response.text\n",
    "        if \"Disallow: /\" in robots_txt:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Fonction pour parser le HTML et extraire les informations (nom, description, lien)\n",
    "def parse_html(soup):\n",
    "    products = []\n",
    "    product_elements = soup.find_all(\"body\")\n",
    "    for element in product_elements:\n",
    "\n",
    "        name = element.find(\"h3\", class_=\"card-title product-title mb-3\")\n",
    "        if name:\n",
    "            name = name.text.strip()\n",
    "        else:\n",
    "            name = soup.find(\"title\").text.strip()\n",
    "\n",
    "        description_text = element.find(\"p\", class_=\"product-description\")\n",
    "        if description_text:\n",
    "            description_text = description_text.text.strip()\n",
    "        else:\n",
    "            description_text = \"\"\n",
    "\n",
    "        features = element.find_all(\"tr\", class_=\"feature\")\n",
    "        links = element.find_all(\"a\")\n",
    "        reviews = element.find_all(\"div\", class_=\"mt-4\")\n",
    "\n",
    "        reviews_list = []\n",
    "        for review in reviews:\n",
    "            classes = review.get(\"class\", [])\n",
    "            review_id = None\n",
    "            for cls in classes:\n",
    "                if cls.startswith(\"review-\") and cls != \"review\":\n",
    "                    review_id = cls.replace(\"review-\", \"\")\n",
    "                    break\n",
    "\n",
    "            rating = len(review.find_all(\"svg\"))\n",
    "\n",
    "            p = review.find(\"p\")\n",
    "            content = p.text.strip() if p else None\n",
    "\n",
    "            reviews_list.append({\n",
    "                \"id\": review_id,\n",
    "                \"rating\": rating,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "        products.append({\n",
    "            \"url\": url,\n",
    "            \"title\": name,            \n",
    "            \"description\": description_text,\n",
    "            \"features\": {feature.find(\"td\", class_=\"feature-label\").text.strip(): feature.find(\"td\", class_=\"feature-value\").text.strip() for feature in features},\n",
    "            \"links\": [link.get(\"href\") for link in links],\n",
    "            \"reviews\": reviews_list\n",
    "        })\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6a0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling URL: https://web-scraping.dev/products\n",
      "<title>web-scraping.dev product page 1</title>\n",
      "[{'url': 'https://web-scraping.dev/products', 'title': 'web-scraping.dev product page 1', 'description': '', 'features': {}, 'links': ['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', None, 'https://web-scraping.dev/products?category=apparel', 'https://web-scraping.dev/products?category=consumables', 'https://web-scraping.dev/products?category=household', 'https://web-scraping.dev/product/1', 'https://web-scraping.dev/product/2', 'https://web-scraping.dev/product/3', 'https://web-scraping.dev/product/4', 'https://web-scraping.dev/product/5', 'https://web-scraping.dev/products?page=1', None, 'https://web-scraping.dev/products?page=2', 'https://web-scraping.dev/products?page=3', 'https://web-scraping.dev/products?page=4', 'https://web-scraping.dev/products?page=5', 'https://web-scraping.dev/products?page=2', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/'], 'reviews': []}]\n",
      "1\n",
      "Crawling URL: https://web-scraping.dev/products?category=apparel\n",
      "<title>web-scraping.dev product page 1</title>\n",
      "[{'url': 'https://web-scraping.dev/products?category=apparel', 'title': 'web-scraping.dev product page 1', 'description': '', 'features': {}, 'links': ['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', 'https://web-scraping.dev/products', None, 'https://web-scraping.dev/products?category=consumables', 'https://web-scraping.dev/products?category=household', 'https://web-scraping.dev/product/7', 'https://web-scraping.dev/product/8', 'https://web-scraping.dev/product/9', 'https://web-scraping.dev/product/10', 'https://web-scraping.dev/product/11', 'https://web-scraping.dev/products?category=apparel&page=1', None, 'https://web-scraping.dev/products?category=apparel&page=2', 'https://web-scraping.dev/products?category=apparel&page=3', 'https://web-scraping.dev/products?category=apparel&page=2', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/'], 'reviews': []}]\n",
      "2\n",
      "Crawling URL: https://web-scraping.dev/products?category=consumables\n",
      "<title>web-scraping.dev product page 1</title>\n",
      "[{'url': 'https://web-scraping.dev/products?category=consumables', 'title': 'web-scraping.dev product page 1', 'description': '', 'features': {}, 'links': ['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', 'https://web-scraping.dev/products', 'https://web-scraping.dev/products?category=apparel', None, 'https://web-scraping.dev/products?category=household', 'https://web-scraping.dev/product/1', 'https://web-scraping.dev/product/2', 'https://web-scraping.dev/product/3', 'https://web-scraping.dev/product/4', 'https://web-scraping.dev/product/5', 'https://web-scraping.dev/products?category=consumables&page=1', None, 'https://web-scraping.dev/products?category=consumables&page=2', 'https://web-scraping.dev/products?category=consumables&page=3', 'https://web-scraping.dev/products?category=consumables&page=4', 'https://web-scraping.dev/products?category=consumables&page=2', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/'], 'reviews': []}]\n",
      "3\n",
      "Crawling URL: https://web-scraping.dev/products?category=household\n",
      "<title>web-scraping.dev product page 1</title>\n",
      "[{'url': 'https://web-scraping.dev/products?category=household', 'title': 'web-scraping.dev product page 1', 'description': '', 'features': {}, 'links': ['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', 'https://web-scraping.dev/products', 'https://web-scraping.dev/products?category=apparel', 'https://web-scraping.dev/products?category=consumables', None, 'https://web-scraping.dev/products?category=household&page=1', 'https://web-scraping.dev/products?category=household&page=2', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/'], 'reviews': []}]\n",
      "4\n",
      "Crawling URL: https://web-scraping.dev/product/1\n",
      "<title>web-scraping.dev product Box of Chocolate Candy</title>\n",
      "[{'url': 'https://web-scraping.dev/product/1', 'title': 'Box of Chocolate Candy', 'description': \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\", 'features': {'material': 'Premium quality chocolate', 'flavors': 'Available in Orange and Cherry flavors', 'sizes': 'Available in small, medium, and large boxes', 'brand': 'ChocoDelight', 'care instructions': 'Store in a cool, dry place', 'purpose': 'Ideal for gifting or self-indulgence'}, 'links': ['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', '/', '/products', 'https://web-scraping.dev/product/1?variant=orange-small', 'https://web-scraping.dev/product/1?variant=orange-medium', 'https://web-scraping.dev/product/1?variant=orange-large', 'https://web-scraping.dev/product/1?variant=cherry-small', 'https://web-scraping.dev/product/1?variant=cherry-medium', 'https://web-scraping.dev/product/1?variant=cherry-large', 'https://web-scraping.dev/product/3', 'https://web-scraping.dev/product/10', 'https://web-scraping.dev/product/24', 'https://web-scraping.dev/product/27', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/'], 'reviews': [{'id': None, 'rating': 0, 'content': None}]}]\n",
      "5\n",
      "Reached maximum number of pages: 5\n"
     ]
    }
   ],
   "source": [
    "url = \"https://web-scraping.dev/products\"\n",
    "\n",
    "output = []\n",
    "urls_priority = []\n",
    "urls_non_priority = []\n",
    "urls_priority.append(url)\n",
    "nb_pages = 5\n",
    "i = 0\n",
    "\n",
    "\n",
    "for url in urls_priority:\n",
    "\n",
    "    if i >= nb_pages:\n",
    "        print(\"Reached maximum number of pages:\", nb_pages)\n",
    "        break\n",
    "    \n",
    "    print(\"Crawling URL:\", url)\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        print(f\"Skipping invalid URL: {url}\")\n",
    "        continue\n",
    "    if can_crawl(url):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(soup.title)\n",
    "        products = parse_html(soup)\n",
    "        list_url = products[0]['links']\n",
    "        for url in list_url:\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"product\" in url and url not in urls_priority:\n",
    "                urls_priority.append(url)\n",
    "            elif url not in urls_non_priority:\n",
    "                urls_non_priority.append(url)\n",
    "        print(products)\n",
    "        output.extend(products)\n",
    "        i += 1\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"Crawling not allowed for this URL.\", url)\n",
    "\n",
    "products_json = json.dumps(output, ensure_ascii=False, indent=4)\n",
    "with open(\"products.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(products_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
