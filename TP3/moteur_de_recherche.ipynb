{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf240d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from urllib.parse import urlparse, urljoin, parse_qs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bec515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbb895",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7938d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    \"\"\"Load JSON data from a file.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict or list: The loaded JSON data.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc94c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_products(path):\n",
    "    \"\"\"Load products from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the JSONL file containing product data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of product dictionaries.\n",
    "    \"\"\"\n",
    "    products = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            products.append(json.loads(line))\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0700bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les index depuis des fichiers dans le dossier input\n",
    "def load_index(index_name):\n",
    "    \"\"\"Load an inverted index from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        index_name (str): The name of the index (e.g., 'title', 'description').\n",
    "\n",
    "    Returns:\n",
    "        dict: The inverted index where keys are tokens and values are dicts of doc_id to positions.\n",
    "    \"\"\"\n",
    "    index_path = os.path.join('input', f'{index_name}_index.json')\n",
    "    with open(index_path, 'r') as f:\n",
    "        raw_index = json.load(f)\n",
    "    \n",
    "    index = {}\n",
    "    for token, doc_list in raw_index.items():\n",
    "        if isinstance(doc_list, list):\n",
    "            index[token] = {doc_id: [1] for doc_id in doc_list}\n",
    "        else:\n",
    "            index[token] = doc_list\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d827e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_indexes(index_dir):\n",
    "    \"\"\"Load all inverted indexes from JSON files in a directory.\n",
    "\n",
    "    Args:\n",
    "        index_dir (str): The directory path containing the index JSON files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are field names and values are inverted indexes.\n",
    "    \"\"\"\n",
    "    indexes = {}\n",
    "    for file in os.listdir(index_dir):\n",
    "        if file.endswith(\"_index.json\"):\n",
    "            field = file.replace(\"_index.json\", \"\")\n",
    "            indexes[field] = load_index(field)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synonyms(path):\n",
    "    \"\"\"Load synonyms from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the synonyms JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of synonyms where keys are words and values are lists of synonyms.\n",
    "    \"\"\"\n",
    "    return load_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de516b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_dict(documents_list):\n",
    "    \"\"\"Convert raw product JSON documents into a dictionary indexed by URL.\n",
    "\n",
    "    Args:\n",
    "        documents_list (list): A list of product dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are URLs and values are processed document info.\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "\n",
    "    for doc in documents_list:\n",
    "        doc_id = doc[\"url\"]\n",
    "\n",
    "        features_text = \" \".join(\n",
    "            f\"{k} {v}\" for k, v in doc.get(\"product_features\", {}).items()\n",
    "        )\n",
    "\n",
    "        full_text = \" \".join([\n",
    "            doc.get(\"title\", \"\"),\n",
    "            doc.get(\"description\", \"\"),\n",
    "            features_text\n",
    "        ]).strip()\n",
    "\n",
    "        documents[doc_id] = {\n",
    "            \"title\": doc.get(\"title\", \"\"),\n",
    "            \"description\": doc.get(\"description\", \"\"),\n",
    "            \"text\": full_text,\n",
    "            \"reviews\": len(doc.get(\"product_reviews\", []))\n",
    "        }\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9993a9e",
   "metadata": {},
   "source": [
    "# Document filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1be5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les documents : Vérifier si au moins un des tokens de la requête est présent dans les documents indexés\n",
    "def filter_token(query_tokens, indexes):\n",
    "    \"\"\"Filter documents where at least one query token is present in any index.\n",
    "\n",
    "    Args:\n",
    "        query_tokens (list): List of query tokens.\n",
    "        indexes (dict): Dictionary of inverted indexes.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of relevant document IDs.\n",
    "    \"\"\"\n",
    "    relevant_docs = set()\n",
    "\n",
    "    for token in query_tokens:\n",
    "        for index in indexes.values():\n",
    "            if token in index:\n",
    "                relevant_docs.update(index[token])\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2881041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les documents : Vérifier si tous les tokens de la requête sont présents dans les documents indexés, sauf les stop words\n",
    "# Renvoie l'ensemble des documents pertinents\n",
    "def filter_all_tokens(query_tokens, indexes):\n",
    "    \"\"\"Filter documents where all query tokens are present in the indexes.\n",
    "\n",
    "    Args:\n",
    "        query_tokens (list): List of query tokens.\n",
    "        indexes (dict): Dictionary of inverted indexes.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of relevant document IDs where all tokens are present.\n",
    "    \"\"\"\n",
    "    relevant_docs = None\n",
    "\n",
    "    for token in query_tokens:\n",
    "        docs_for_token = set()\n",
    "\n",
    "        for index in indexes.values():\n",
    "            if token in index:\n",
    "                docs_for_token |= set(index[token])\n",
    "                \n",
    "        if not docs_for_token:\n",
    "            return set()\n",
    "\n",
    "        if relevant_docs is None:\n",
    "            relevant_docs = docs_for_token\n",
    "        else:\n",
    "            relevant_docs &= docs_for_token\n",
    "\n",
    "    if relevant_docs is None:\n",
    "        return set()\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9808a",
   "metadata": {},
   "source": [
    "# Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp pour traiter les querys\n",
    "def tokenize_normalize_text(text):\n",
    "    \"\"\"Tokenize and normalize text by lowercasing, removing stop words, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        list: List of normalized tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour traiter les synonymes dans la query\n",
    "def expand_query_with_synonyms(tokens, synonyms):\n",
    "    \"\"\"Expand query tokens using synonym dictionary.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of query tokens.\n",
    "        synonyms (dict): Dictionary of synonyms.\n",
    "\n",
    "    Returns:\n",
    "        list: Expanded list of tokens including synonyms.\n",
    "    \"\"\"\n",
    "    expanded = set(tokens)\n",
    "    for token in tokens:\n",
    "        if token in synonyms:\n",
    "            expanded.update(synonyms[token])\n",
    "    return list(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour traiter les querys\n",
    "def process_query(query, stopwords, synonyms):\n",
    "    \"\"\"Full query processing pipeline: tokenize, remove stopwords, expand synonyms.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        stopwords (set): Set of stopwords to remove.\n",
    "        synonyms (dict): Dictionary of synonyms.\n",
    "\n",
    "    Returns:\n",
    "        list: Processed query tokens.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_normalize_text(query)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    tokens = expand_query_with_synonyms(tokens, synonyms)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200864a",
   "metadata": {},
   "source": [
    "# Ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a257056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(token, inverted_index, total_docs):\n",
    "    \"\"\"Compute inverse document frequency for a token.\n",
    "\n",
    "    Args:\n",
    "        token (str): The token to compute IDF for.\n",
    "        inverted_index (dict): The inverted index.\n",
    "        total_docs (int): Total number of documents.\n",
    "\n",
    "    Returns:\n",
    "        float: The IDF score.\n",
    "    \"\"\"\n",
    "    df = len(inverted_index.get(token, {}))\n",
    "    return math.log((total_docs + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour le score bm25\n",
    "def bm25_score(doc_id, \n",
    "               tokens, \n",
    "               inverted_index, \n",
    "               doc_lengths, \n",
    "               avg_doc_length, \n",
    "               k1, \n",
    "               b):\n",
    "    \"\"\"Compute BM25 score for a document.\n",
    "\n",
    "    Args:\n",
    "        doc_id (str): The document ID.\n",
    "        tokens (list): List of query tokens.\n",
    "        inverted_index (dict): The inverted index.\n",
    "        doc_lengths (dict): Dictionary of document lengths.\n",
    "        avg_doc_length (float): Average document length.\n",
    "        k1 (float): BM25 parameter k1.\n",
    "        b (float): BM25 parameter b.\n",
    "\n",
    "    Returns:\n",
    "        float: The BM25 score.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            continue\n",
    "        tf = len(inverted_index[token].get(doc_id, []))\n",
    "        idf = compute_idf(token, inverted_index, len(doc_lengths))\n",
    "        denom = tf + k1 * (1 - b + b * doc_lengths[doc_id] / avg_doc_length)\n",
    "        score += idf * ((tf * (k1 + 1)) / (denom + 1e-9))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbadec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_score(doc_text, query):\n",
    "    \"\"\"Score exact query match in document text.\n",
    "\n",
    "    Args:\n",
    "        doc_text (str): The document text.\n",
    "        query (str): The query string.\n",
    "\n",
    "    Returns:\n",
    "        float: 1.0 if query matches exactly, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    return float(query.lower() in doc_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d586324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_score(features, weights):\n",
    "    \"\"\"Combine multiple features using a linear model.\n",
    "\n",
    "    Args:\n",
    "        features (dict): Dictionary of feature names to values.\n",
    "        weights (dict): Dictionary of feature names to weights.\n",
    "\n",
    "    Returns:\n",
    "        float: The combined score.\n",
    "    \"\"\"\n",
    "    return sum(features[name] * weights.get(name, 0.0)\n",
    "               for name in features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cacaf",
   "metadata": {},
   "source": [
    "# Search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19db8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(doc_ids,\n",
    "                   query_tokens,\n",
    "                   indexes,\n",
    "                   metadata,\n",
    "                   weights):\n",
    "    \"\"\"Rank filtered documents using BM25 and other features.\n",
    "\n",
    "    Args:\n",
    "        doc_ids (set): Set of document IDs to rank.\n",
    "        query_tokens (list): List of query tokens.\n",
    "        indexes (dict): Dictionary of inverted indexes.\n",
    "        metadata (dict): Metadata including doc_lengths, avg_doc_length, documents.\n",
    "        weights (dict): Weights for different features.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (doc_id, score) tuples, sorted by score descending.\n",
    "    \"\"\"\n",
    "    ranked = []\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        features = {}\n",
    "\n",
    "        for field, index in indexes.items():\n",
    "            features[field] = bm25_score(\n",
    "                doc_id,\n",
    "                query_tokens,\n",
    "                index,\n",
    "                metadata[\"doc_lengths\"],\n",
    "                metadata[\"avg_doc_length\"],\n",
    "                1.5,  # k1\n",
    "                0.75  # b\n",
    "            )\n",
    "\n",
    "        features[\"exact_match\"] = exact_match_score(\n",
    "            metadata[\"documents\"][doc_id][\"text\"],\n",
    "            \" \".join(query_tokens)\n",
    "        )\n",
    "\n",
    "        features[\"reviews\"] = metadata[\"documents\"][doc_id].get(\"reviews\", 0)\n",
    "\n",
    "        score = compute_linear_score(features, weights)\n",
    "        ranked.append((doc_id, score))\n",
    "\n",
    "    return sorted(ranked, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4072f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(ranked_docs,\n",
    "                   documents,\n",
    "                   total_docs,\n",
    "                   filtered_docs,\n",
    "                   limit,\n",
    "                   query) -> dict:\n",
    "    \"\"\"Format search results as JSON.\n",
    "\n",
    "    Args:\n",
    "        ranked_docs (list): List of (doc_id, score) tuples.\n",
    "        documents (dict): Dictionary of documents.\n",
    "        total_docs (int): Total number of documents.\n",
    "        filtered_docs (int): Number of filtered documents.\n",
    "        limit (int): Maximum number of results to return.\n",
    "        query (str): The original search query.\n",
    "\n",
    "    Returns:\n",
    "        dict: Formatted results with metadata and results list.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc_id, score in ranked_docs[:limit]:\n",
    "        doc = documents[doc_id]\n",
    "        results.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"url\": doc_id,\n",
    "            \"description\": doc[\"description\"],\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"metadata\": {\n",
    "            \"total_documents\": total_docs,\n",
    "            \"filtered_documents\": filtered_docs\n",
    "        },\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ead18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query,\n",
    "           indexes,\n",
    "           documents,\n",
    "           stopwords,\n",
    "           synonyms,\n",
    "           metadata,\n",
    "           weights) -> dict:\n",
    "    \"\"\"Execute a full search pipeline.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        indexes (dict): Dictionary of inverted indexes.\n",
    "        documents (dict): Dictionary of documents.\n",
    "        stopwords (set): Set of stopwords.\n",
    "        synonyms (dict): Dictionary of synonyms.\n",
    "        metadata (dict): Metadata for ranking.\n",
    "        weights (dict): Weights for scoring.\n",
    "\n",
    "    Returns:\n",
    "        dict: Search results.\n",
    "    \"\"\"\n",
    "    query_tokens = process_query(query, stopwords, synonyms)\n",
    "\n",
    "    filtered_docs = filter_all_tokens(query_tokens, indexes)\n",
    "\n",
    "    if not filtered_docs:\n",
    "        filtered_docs = filter_token(query_tokens, indexes)\n",
    "\n",
    "    ranked_docs = rank_documents(\n",
    "        filtered_docs,\n",
    "        query_tokens,\n",
    "        indexes,\n",
    "        metadata,\n",
    "        weights\n",
    "    )\n",
    "\n",
    "    return format_results(\n",
    "        ranked_docs,\n",
    "        documents,\n",
    "        total_docs=len(documents),\n",
    "        filtered_docs=len(filtered_docs),\n",
    "        limit=156,\n",
    "        query=query\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10f28b",
   "metadata": {},
   "source": [
    "# Metadonnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab985e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_lengths(documents):\n",
    "    \"\"\"Compute the length of each document in words.\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of doc_id to document length.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        doc_id: len(doc[\"text\"].split())\n",
    "        for doc_id, doc in documents.items()\n",
    "    }\n",
    "\n",
    "def compute_avg_doc_length(doc_lengths):\n",
    "    \"\"\"Compute the average document length.\n",
    "\n",
    "    Args:\n",
    "        doc_lengths (dict): Dictionary of document lengths.\n",
    "\n",
    "    Returns:\n",
    "        float: The average document length.\n",
    "    \"\"\"\n",
    "    return sum(doc_lengths.values()) / len(doc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59fa2e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeu de requêtes test\n",
    "TEST_QUERIES = [\n",
    "    \"chocolate candy\",\n",
    "    \"leather sneakers\",\n",
    "    \"italy\",\n",
    "    \"brazil\",\n",
    "    \"timelessfootwear\",\n",
    "    \"premium chocolate\",\n",
    "    \"comfortable shoes\",\n",
    "    \"light up sneaker\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = load_all_indexes('input')\n",
    "documents = load_products(os.path.join('rearranged_products.jsonl'))\n",
    "documents = build_documents_dict(documents)\n",
    "synonyms = load_synonyms(os.path.join('input', 'origin_synonyms.json'))\n",
    "\n",
    "\n",
    "doc_lengths = compute_doc_lengths(documents)\n",
    "\n",
    "metadata = {\n",
    "    \"doc_lengths\": doc_lengths,\n",
    "    \"avg_doc_length\": compute_avg_doc_length(doc_lengths),\n",
    "    \"documents\": documents\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    \"title\": 2.0,           # Higher weight for title matches (titles are usually more important)\n",
    "    \"description\": 1.0,     # Standard weight for description matches\n",
    "    \"brand\": 1.5,           # Slightly higher weight for brand names (brand recognition matters)\n",
    "    \"origin\": 0.5,          # Lower weight for country of origin (less relevant for most queries)\n",
    "    \"reviews\": 0.1,         # Small weight for review text matches (secondary relevance)\n",
    "    \"bm25\": 1.0,            # Standard BM25 score for term frequency and document length normalization\n",
    "    \"exact_match\": 1.5      # Higher weight for exact query matches (strong relevance signal)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ed51a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "QUERY: chocolate candy\n",
      "- Box of Chocolate Candy | score: 15.754654065901747\n",
      "- Box of Chocolate Candy | score: 15.632826115582338\n",
      "- Box of Chocolate Candy - Cherry large | score: 15.572738682030465\n",
      "Fichier existant 'results_query_1.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_1.json\n",
      "==================================================\n",
      "QUERY: leather sneakers\n",
      "- Classic Leather Sneakers - White40 | score: 9.575675352223332\n",
      "- Classic Leather Sneakers - White40 | score: 9.575675352223332\n",
      "- Classic Leather Sneakers - White41 | score: 9.575675352223332\n",
      "Fichier existant 'results_query_2.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_2.json\n",
      "==================================================\n",
      "QUERY: light up sneakers\n",
      "Fichier existant 'results_query_3.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_3.json\n",
      "==================================================\n",
      "QUERY: italy\n",
      "- web-scraping.dev product page 3 | score: 3.1803649180125158\n",
      "- web-scraping.dev product page 2 | score: 3.1803649180125158\n",
      "- Box of Chocolate Candy - Cherry medium | score: 2.9711958428247756\n",
      "Fichier existant 'results_query_4.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_4.json\n",
      "==================================================\n",
      "QUERY: brazil\n",
      "- web-scraping.dev product page 2 | score: 4.159520662218369\n",
      "- Box of Chocolate Candy | score: 3.6592540710893218\n",
      "- Red Energy Potion - One | score: 3.604397542256168\n",
      "Fichier existant 'results_query_5.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_5.json\n",
      "==================================================\n",
      "QUERY: timelessfootwear\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White41 | score: 5.287794741296978\n",
      "Fichier existant 'results_query_6.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_6.json\n",
      "==================================================\n",
      "QUERY: premium chocolate\n",
      "- Box of Chocolate Candy | score: 7.377327032950873\n",
      "- Box of Chocolate Candy | score: 7.316413057791169\n",
      "- Box of Chocolate Candy - Cherry large | score: 7.286369341015233\n",
      "Fichier existant 'results_query_7.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_7.json\n",
      "==================================================\n",
      "QUERY: comfortable shoes\n",
      "- Women's High Heel Sandals - Beige 8 | score: 2.1508595389774605\n",
      "- Women's High Heel Sandals - Beige 6 | score: 2.1508595389774605\n",
      "- Running Shoes for Men - 12 | score: 2.1508595389774605\n",
      "Fichier existant 'results_query_8.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_8.json\n",
      "- Box of Chocolate Candy | score: 15.754654065901747\n",
      "- Box of Chocolate Candy | score: 15.632826115582338\n",
      "- Box of Chocolate Candy - Cherry large | score: 15.572738682030465\n",
      "Fichier existant 'results_query_1.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_1.json\n",
      "==================================================\n",
      "QUERY: leather sneakers\n",
      "- Classic Leather Sneakers - White40 | score: 9.575675352223332\n",
      "- Classic Leather Sneakers - White40 | score: 9.575675352223332\n",
      "- Classic Leather Sneakers - White41 | score: 9.575675352223332\n",
      "Fichier existant 'results_query_2.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_2.json\n",
      "==================================================\n",
      "QUERY: light up sneakers\n",
      "Fichier existant 'results_query_3.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_3.json\n",
      "==================================================\n",
      "QUERY: italy\n",
      "- web-scraping.dev product page 3 | score: 3.1803649180125158\n",
      "- web-scraping.dev product page 2 | score: 3.1803649180125158\n",
      "- Box of Chocolate Candy - Cherry medium | score: 2.9711958428247756\n",
      "Fichier existant 'results_query_4.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_4.json\n",
      "==================================================\n",
      "QUERY: brazil\n",
      "- web-scraping.dev product page 2 | score: 4.159520662218369\n",
      "- Box of Chocolate Candy | score: 3.6592540710893218\n",
      "- Red Energy Potion - One | score: 3.604397542256168\n",
      "Fichier existant 'results_query_5.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_5.json\n",
      "==================================================\n",
      "QUERY: timelessfootwear\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White41 | score: 5.287794741296978\n",
      "Fichier existant 'results_query_6.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_6.json\n",
      "==================================================\n",
      "QUERY: premium chocolate\n",
      "- Box of Chocolate Candy | score: 7.377327032950873\n",
      "- Box of Chocolate Candy | score: 7.316413057791169\n",
      "- Box of Chocolate Candy - Cherry large | score: 7.286369341015233\n",
      "Fichier existant 'results_query_7.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_7.json\n",
      "==================================================\n",
      "QUERY: comfortable shoes\n",
      "- Women's High Heel Sandals - Beige 8 | score: 2.1508595389774605\n",
      "- Women's High Heel Sandals - Beige 6 | score: 2.1508595389774605\n",
      "- Running Shoes for Men - 12 | score: 2.1508595389774605\n",
      "Fichier existant 'results_query_8.json' supprimé.\n",
      "Résultats sauvegardés dans output/results_query_8.json\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(TEST_QUERIES, 1):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"QUERY:\", q)\n",
    "    results = search(\n",
    "        query=q,\n",
    "        indexes=indexes,\n",
    "        documents=documents,\n",
    "        stopwords=stop_words,\n",
    "        synonyms=synonyms,\n",
    "        metadata=metadata,\n",
    "        weights=weights\n",
    "    )\n",
    "    for r in results[\"results\"][:3]:\n",
    "        print(\"-\", r[\"title\"], \"| score:\", r[\"score\"])\n",
    "\n",
    "    file_name = f\"results_query_{i}.json\"\n",
    "    file_path = os.path.join(\"output\", file_name)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Fichier existant '{file_name}' supprimé.\")\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Résultats sauvegardés dans {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
