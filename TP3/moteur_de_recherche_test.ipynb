{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "daf240d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from urllib.parse import urlparse, urljoin, parse_qs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10bec515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ensai/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbb895",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7938d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc94c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_products(path):\n",
    "    products = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            products.append(json.loads(line))\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b0700bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les index depuis des fichiers dans le dossier input\n",
    "def load_index(index_name):\n",
    "    index_path = os.path.join('input', f'{index_name}_index.json')\n",
    "    with open(index_path, 'r') as f:\n",
    "        raw_index = json.load(f)\n",
    "    \n",
    "    index = {}\n",
    "    for token, doc_list in raw_index.items():\n",
    "        if isinstance(doc_list, list):\n",
    "            index[token] = {doc_id: [1] for doc_id in doc_list}\n",
    "        else:\n",
    "            index[token] = doc_list\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d827e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_indexes(index_dir):\n",
    "    indexes = {}\n",
    "    for file in os.listdir(index_dir):\n",
    "        if file.endswith(\"_index.json\"):\n",
    "            field = file.replace(\"_index.json\", \"\")\n",
    "            indexes[field] = load_index(field)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79c429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synonyms(path):\n",
    "    \"\"\"\n",
    "    Load synonyms from a JSON file.\n",
    "    \"\"\"\n",
    "    return load_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de516b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_dict(documents_list):\n",
    "    \"\"\"\n",
    "    Convert raw product JSON documents into a dictionary indexed by URL.\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "\n",
    "    for doc in documents_list:\n",
    "        doc_id = doc[\"url\"]\n",
    "\n",
    "        features_text = \" \".join(\n",
    "            f\"{k} {v}\" for k, v in doc.get(\"product_features\", {}).items()\n",
    "        )\n",
    "\n",
    "        full_text = \" \".join([\n",
    "            doc.get(\"title\", \"\"),\n",
    "            doc.get(\"description\", \"\"),\n",
    "            features_text\n",
    "        ]).strip()\n",
    "\n",
    "        documents[doc_id] = {\n",
    "            \"title\": doc.get(\"title\", \"\"),\n",
    "            \"description\": doc.get(\"description\", \"\"),\n",
    "            \"text\": full_text,\n",
    "            \"reviews\": len(doc.get(\"product_reviews\", []))\n",
    "        }\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9993a9e",
   "metadata": {},
   "source": [
    "# Document filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f1be5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les documents : Vérifier si au moins un des tokens de la requête est présent dans les documents indexés\n",
    "def filter_token(query_tokens, indexes):\n",
    "    relevant_docs = set()\n",
    "\n",
    "    for token in query_tokens:\n",
    "        for index in indexes.values():\n",
    "            if token in index:\n",
    "                relevant_docs.update(index[token])\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2881041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les documents : Vérifier si tous les tokens de la requête sont présents dans les documents indexés, sauf les stop words\n",
    "# Renvoie l'ensemble des documents pertinents\n",
    "def filter_all_tokens(query_tokens, indexes):\n",
    "    relevant_docs = None\n",
    "\n",
    "    for token in query_tokens:\n",
    "        docs_for_token = set()\n",
    "\n",
    "        for index in indexes.values():\n",
    "            if token in index:\n",
    "                docs_for_token |= set(index[token])\n",
    "                \n",
    "        if not docs_for_token:\n",
    "            return set()\n",
    "\n",
    "        if relevant_docs is None:\n",
    "            relevant_docs = docs_for_token\n",
    "        else:\n",
    "            relevant_docs &= docs_for_token\n",
    "\n",
    "    if relevant_docs is None:\n",
    "        return set()\n",
    "\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9808a",
   "metadata": {},
   "source": [
    "# Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e33af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_normalize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize and normalize text.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "435b2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_synonyms(tokens, synonyms):\n",
    "    \"\"\"\n",
    "    Expand query tokens using synonym dictionary.\n",
    "    \"\"\"\n",
    "    expanded = set(tokens)\n",
    "    for token in tokens:\n",
    "        if token in synonyms:\n",
    "            expanded.update(synonyms[token])\n",
    "    return list(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bc4a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, stopwords, synonyms):\n",
    "    \"\"\"\n",
    "    Full query processing pipeline.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_normalize_text(query)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    tokens = expand_query_with_synonyms(tokens, synonyms)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200864a",
   "metadata": {},
   "source": [
    "# Ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a257056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(token, inverted_index, total_docs):\n",
    "    \"\"\"\n",
    "    Compute inverse document frequency.\n",
    "    \"\"\"\n",
    "    df = len(inverted_index.get(token, {}))\n",
    "    return math.log((total_docs + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e2fbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score(doc_id, tokens, inverted_index, doc_lengths, avg_doc_length, k1, b):\n",
    "    \"\"\"\n",
    "    Compute BM25 score for a document.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            continue\n",
    "        tf = len(inverted_index[token].get(doc_id, []))\n",
    "        idf = compute_idf(token, inverted_index, len(doc_lengths))\n",
    "        denom = tf + k1 * (1 - b + b * doc_lengths[doc_id] / avg_doc_length)\n",
    "        score += idf * ((tf * (k1 + 1)) / (denom + 1e-9))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fbadec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_score(doc_text, query):\n",
    "    \"\"\"\n",
    "    Score exact query match.\n",
    "    \"\"\"\n",
    "    return float(query.lower() in doc_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d586324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_linear_score(features, weights):\n",
    "    \"\"\"\n",
    "    Combine multiple features using a linear model.\n",
    "    \"\"\"\n",
    "    return sum(features[name] * weights.get(name, 0.0)\n",
    "               for name in features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cacaf",
   "metadata": {},
   "source": [
    "# Search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e19db8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(doc_ids,\n",
    "                   query_tokens,\n",
    "                   indexes,\n",
    "                   metadata,\n",
    "                   weights):\n",
    "    \"\"\"\n",
    "    Rank filtered documents.\n",
    "    \"\"\"\n",
    "    ranked = []\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        features = {}\n",
    "\n",
    "        for field, index in indexes.items():\n",
    "            features[field] = bm25_score(\n",
    "                doc_id,\n",
    "                query_tokens,\n",
    "                index,\n",
    "                metadata[\"doc_lengths\"],\n",
    "                metadata[\"avg_doc_length\"],\n",
    "                1.5,  # k1\n",
    "                0.75  # b\n",
    "            )\n",
    "\n",
    "        features[\"exact_match\"] = exact_match_score(\n",
    "            metadata[\"documents\"][doc_id][\"text\"],\n",
    "            \" \".join(query_tokens)\n",
    "        )\n",
    "\n",
    "        features[\"reviews\"] = metadata[\"documents\"][doc_id].get(\"reviews\", 0)\n",
    "\n",
    "        score = compute_linear_score(features, weights)\n",
    "        ranked.append((doc_id, score))\n",
    "\n",
    "    return sorted(ranked, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4072f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(ranked_docs,\n",
    "                   documents,\n",
    "                   total_docs,\n",
    "                   filtered_docs,\n",
    "                   limit) -> dict:\n",
    "    \"\"\"\n",
    "    Format search results as JSON.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc_id, score in ranked_docs[:limit]:\n",
    "        doc = documents[doc_id]\n",
    "        results.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"url\": doc_id,\n",
    "            \"description\": doc[\"description\"],\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"metadata\": {\n",
    "            \"total_documents\": total_docs,\n",
    "            \"filtered_documents\": filtered_docs\n",
    "        },\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ead18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query,\n",
    "           indexes,\n",
    "           documents,\n",
    "           stopwords,\n",
    "           synonyms,\n",
    "           metadata,\n",
    "           weights) -> dict:\n",
    "    \"\"\"\n",
    "    Execute a full search pipeline.\n",
    "    \"\"\"\n",
    "    query_tokens = process_query(query, stopwords, synonyms)\n",
    "\n",
    "    filtered_docs = filter_all_tokens(query_tokens, indexes)\n",
    "\n",
    "    if not filtered_docs:\n",
    "        filtered_docs = filter_token(query_tokens, indexes)\n",
    "\n",
    "    ranked_docs = rank_documents(\n",
    "        filtered_docs,\n",
    "        query_tokens,\n",
    "        indexes,\n",
    "        metadata,\n",
    "        weights\n",
    "    )\n",
    "\n",
    "    return format_results(\n",
    "        ranked_docs,\n",
    "        documents,\n",
    "        total_docs=len(documents),\n",
    "        filtered_docs=len(filtered_docs),\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10f28b",
   "metadata": {},
   "source": [
    "# Metadonnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab985e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_lengths(documents):\n",
    "    return {\n",
    "        doc_id: len(doc[\"text\"].split())\n",
    "        for doc_id, doc in documents.items()\n",
    "    }\n",
    "\n",
    "def compute_avg_doc_length(doc_lengths):\n",
    "    return sum(doc_lengths.values()) / len(doc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59fa2e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "89a711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeu de requêtes test\n",
    "TEST_QUERIES = [\n",
    "    \"chocolate candy\",\n",
    "    \"leather sneakers\",\n",
    "    \"light up sneakers\",\n",
    "    \"italy\",\n",
    "    \"brazil\",\n",
    "    \"timelessfootwear\",\n",
    "    \"premium chocolate\",\n",
    "    \"comfortable shoes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e40542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = load_all_indexes('input')\n",
    "documents = load_products(os.path.join('rearranged_products.jsonl'))\n",
    "documents = build_documents_dict(documents)\n",
    "synonyms = load_synonyms(os.path.join('input', 'origin_synonyms.json'))\n",
    "\n",
    "\n",
    "doc_lengths = compute_doc_lengths(documents)\n",
    "\n",
    "metadata = {\n",
    "    \"doc_lengths\": doc_lengths,\n",
    "    \"avg_doc_length\": compute_avg_doc_length(doc_lengths),\n",
    "    \"documents\": documents\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    \"title\": 2.0,\n",
    "    \"description\": 1.0,\n",
    "    \"brand\": 1.5,\n",
    "    \"origin\": 0.5,\n",
    "    \"reviews\": 0.1,\n",
    "    \"bm25\": 1.0,\n",
    "    \"exact_match\": 1.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed51a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "QUERY: chocolate candy\n",
      "- Box of Chocolate Candy | score: 14.254654065901747\n",
      "- Box of Chocolate Candy | score: 14.132826115582338\n",
      "- Box of Chocolate Candy - Cherry large | score: 14.072738682030465\n",
      "Résultats sauvegardés dans results_query_1.json\n",
      "==================================================\n",
      "QUERY: leather sneakers\n",
      "- Classic Leather Sneakers - White40 | score: 8.075675352223332\n",
      "- Classic Leather Sneakers - White41 | score: 8.075675352223332\n",
      "- Classic Leather Sneakers - White42 | score: 8.075675352223332\n",
      "Résultats sauvegardés dans results_query_2.json\n",
      "==================================================\n",
      "QUERY: light up sneakers\n",
      "Résultats sauvegardés dans results_query_3.json\n",
      "==================================================\n",
      "QUERY: italy\n",
      "- web-scraping.dev product page 3 | score: 3.1803649180125158\n",
      "- web-scraping.dev product page 2 | score: 3.1803649180125158\n",
      "- Box of Chocolate Candy - Cherry small | score: 2.9711958428247756\n",
      "Résultats sauvegardés dans results_query_4.json\n",
      "==================================================\n",
      "QUERY: brazil\n",
      "- web-scraping.dev product page 2 | score: 4.159520662218369\n",
      "- Box of Chocolate Candy | score: 3.6592540710893218\n",
      "- Red Energy Potion - One | score: 3.604397542256168\n",
      "Résultats sauvegardés dans results_query_5.json\n",
      "==================================================\n",
      "QUERY: timelessfootwear\n",
      "- Classic Leather Sneakers - White42 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "- Classic Leather Sneakers - White40 | score: 5.287794741296978\n",
      "Résultats sauvegardés dans results_query_6.json\n",
      "==================================================\n",
      "QUERY: premium chocolate\n",
      "- Box of Chocolate Candy | score: 7.377327032950873\n",
      "- Box of Chocolate Candy | score: 7.316413057791169\n",
      "- Box of Chocolate Candy - Cherry large | score: 7.286369341015233\n",
      "Résultats sauvegardés dans results_query_7.json\n",
      "==================================================\n",
      "QUERY: comfortable shoes\n",
      "- Running Shoes for Men - 9 | score: 2.1508595389774605\n",
      "- Women's High Heel Sandals - Beige 6 | score: 2.1508595389774605\n",
      "- Women's High Heel Sandals - Beige 8 | score: 2.1508595389774605\n",
      "Résultats sauvegardés dans results_query_8.json\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(TEST_QUERIES, 1):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"QUERY:\", q)\n",
    "    results = search(\n",
    "        query=q,\n",
    "        indexes=indexes,\n",
    "        documents=documents,\n",
    "        stopwords=stop_words,\n",
    "        synonyms=synonyms,\n",
    "        metadata=metadata,\n",
    "        weights=weights\n",
    "    )\n",
    "    for r in results[\"results\"][:3]:\n",
    "        print(\"-\", r[\"title\"], \"| score:\", r[\"score\"])\n",
    "\n",
    "    file_name = f\"results_query_{i}.json\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Résultats sauvegardés dans {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_resault = format_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
